Not improving/learning:
Set the learning rate (alpha) to a very low value, such as 0.01. This will cause the algorithm to make only small updates to the Q-function at each iteration, resulting in slow learning or no improvement.

High variance but fast learning:
Set the learning rate (alpha) to a high value, such as 0.9. This will cause the algorithm to make large updates to the Q-function at each iteration, resulting in fast learning but also high variance in the estimates.

Low variance and high long-term return:
Set the learning rate (alpha) to a moderate value, such as 0.5. This will cause the algorithm to make moderate updates to the Q-function at each iteration, resulting in a balance between fast learning and low variance in the estimates.

High variance and high long-term return:
Set the learning rate (alpha) to a high value, such as 0.9, and set the discount factor (gamma) to a low value, such as 0.1. This will cause the algorithm to prioritize short-term rewards over long-term returns, resulting in high variance in the estimates but also potentially high long-term returns.


However, if the reward structure of an MDP is simple enough, the optimal policy degenerates in a simple heuristic. Given the 4_2_3.yml reward structure and initial position of
jelly sh/king sh/diver, what is the value of the long term return of the optimal policy?


Reduce alpha over time:

To improve the stability of the algorithm: 
As the Q-function is updated iteratively using the Bellman equation, the value of each state can change significantly from one iteration to the next. This can lead to instability in the estimates, particularly if the learning rate is set too high. By decreasing the value of alpha over time, we can reduce the magnitude of the updates made to the Q-function at each iteration, which can help to stabilize the estimates and improve the overall performance of the algorithm.

To allow for more fine-tuned adjustments: 
As the Q-function is updated over time, it may be beneficial to make smaller adjustments to the estimates as the algorithm converges on the optimal policy. By decreasing the value of alpha over time, we can allow the algorithm to make more fine-tuned adjustments to the Q-function, which may be more accurate and lead to better overall performance.

To prevent overfitting: 
If the learning rate is set too high, the Q-function may become overfitted to the training data and may not generalize well to new situations. By decreasing the value of alpha over time, we can reduce the risk of overfitting and improve the generalization capabilities of the algorithm.